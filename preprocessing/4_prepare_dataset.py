"""
Prepare final dataset: integrate A-class and B-class data, split into train/val sets (8:2)
"""
import json
import shutil
from pathlib import Path
import cv2
from tqdm import tqdm
import random

CONFIG_FILE = Path(__file__).parent / "config.json"

def load_config():
    if not CONFIG_FILE.exists():
        raise FileNotFoundError(f"配置文件不存在: {CONFIG_FILE}")
    
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config

config = load_config()
ORGANIZED_DIR = Path(config['paths']['organized_dir'])
DATASET_DIR = Path(config['paths']['dataset_dir'])
B_CLASS_DIR = Path(config['paths']['b_class_dir'])
OUTPUT_SIZE = tuple(config['processing']['output_size'])

def prepare_a_class_data(organized_dir, dataset_dir):
    """Prepare A-class data (structured light data)"""
    index_file = organized_dir / "data_index.json"
    
    if not index_file.exists():
        print("Error: please run previous scripts to generate data index")
        return []
    
    with open(index_file, 'r', encoding='utf-8') as f:
        data_index = json.load(f)
    
    a_class_dir = dataset_dir / "A_class"
    a_class_dir.mkdir(parents=True, exist_ok=True)
    
    a_samples = []
    all_samples = []
    
    for label in ['wild', 'farmed']:
        for sample_info in data_index[label]:
            sample_id = sample_info['sample_id']
            source_dir = organized_dir / label / sample_id
            
            rgb_file = source_dir / "rgb_256.png"
            depth_file = source_dir / "depth_256.npy"
            mask_file = source_dir / "mask_256.png"
            
            if not (rgb_file.exists() and depth_file.exists() and mask_file.exists()):
                print(f"Warning: {sample_id} missing required files, skipping")
                continue
            
            dest_dir = a_class_dir / label / sample_id
            dest_dir.mkdir(parents=True, exist_ok=True)
            
            shutil.copy2(rgb_file, dest_dir / "rgb.png")
            shutil.copy2(depth_file, dest_dir / "depth.npy")
            shutil.copy2(mask_file, dest_dir / "mask.png")
            
            sample_record = {
                'sample_id': sample_id,
                'label': label,
                'label_id': 0 if label == 'wild' else 1,
                'rgb': f"A_class/{label}/{sample_id}/rgb.png",
                'depth': f"A_class/{label}/{sample_id}/depth.npy",
                'mask': f"A_class/{label}/{sample_id}/mask.png",
                'has_depth': True
            }
            
            a_samples.append(sample_record)
            all_samples.append(sample_record)
    
    print(f"A-class data prepared: {len(a_samples)} samples")
    print(f"  Wild: {sum(1 for s in a_samples if s['label'] == 'wild')}")
    print(f"  Farmed: {sum(1 for s in a_samples if s['label'] == 'farmed')}")
    
    return all_samples

def prepare_b_class_data(dataset_dir):
    """Prepare B-class data (read from index generated by 5_prepare_b_class_data.py)"""
    b_class_index_file = dataset_dir / "B_class_index.json"
    
    if not b_class_index_file.exists():
        print("Note: B-class data index file not found")
        print("      Please run: python 5_prepare_b_class_data.py")
        return []
    
    with open(b_class_index_file, 'r', encoding='utf-8') as f:
        b_class_index = json.load(f)
    
    all_samples = []
    for label in ['wild', 'farmed']:
        all_samples.extend(b_class_index.get(label, []))
    
    print(f"\nB-class data prepared: {len(all_samples)} samples")
    print(f"  Wild: {sum(1 for s in all_samples if s['label'] == 'wild')}")
    print(f"  Farmed: {sum(1 for s in all_samples if s['label'] == 'farmed')}")
    
    return all_samples

def split_dataset(samples, train_ratio=0.8, random_seed=42):
    """Split dataset into train and validation sets"""
    if len(samples) == 0:
        return [], []
    
    random.seed(random_seed)
    shuffled_samples = samples.copy()
    random.shuffle(shuffled_samples)
    
    split_idx = int(len(shuffled_samples) * train_ratio)
    train_samples = shuffled_samples[:split_idx]
    val_samples = shuffled_samples[split_idx:]
    
    return train_samples, val_samples

def create_dataset_index(all_samples, dataset_dir, train_ratio=0.8):
    """Create dataset index file with 8:2 train/val split"""
    grouped_samples = {
        'A_class': {
            'wild': [],
            'farmed': []
        },
        'B_class': {
            'wild': [],
            'farmed': []
        }
    }
    
    for sample in all_samples:
        if sample['has_depth']:
            grouped_samples['A_class'][sample['label']].append(sample)
        else:
            grouped_samples['B_class'][sample['label']].append(sample)
    
    dataset_index = {
        'A_class': {
            'train': {'wild': [], 'farmed': []},
            'val': {'wild': [], 'farmed': []}
        },
        'B_class': {
            'train': {'wild': [], 'farmed': []},
            'val': {'wild': [], 'farmed': []}
        }
    }
    
    for data_class in ['A_class', 'B_class']:
        for label in ['wild', 'farmed']:
            samples = grouped_samples[data_class][label]
            if len(samples) > 0:
                train_samples, val_samples = split_dataset(samples, train_ratio)
                dataset_index[data_class]['train'][label] = train_samples
                dataset_index[data_class]['val'][label] = val_samples
    
    index_file = dataset_dir / "dataset_index.json"
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(dataset_index, f, indent=2, ensure_ascii=False)
    
    print(f"\nDataset index saved: {index_file}")
    
    print("\nDataset statistics (8:2 split):")
    print("\nA-class data:")
    a_wild_train = len(dataset_index['A_class']['train']['wild'])
    a_wild_val = len(dataset_index['A_class']['val']['wild'])
    a_farmed_train = len(dataset_index['A_class']['train']['farmed'])
    a_farmed_val = len(dataset_index['A_class']['val']['farmed'])
    print(f"  Wild - Train: {a_wild_train}, Val: {a_wild_val}, Total: {a_wild_train + a_wild_val}")
    print(f"  Farmed - Train: {a_farmed_train}, Val: {a_farmed_val}, Total: {a_farmed_train + a_farmed_val}")
    print(f"  A-class Total - Train: {a_wild_train + a_farmed_train}, Val: {a_wild_val + a_farmed_val}")
    
    print("\nB-class data:")
    b_wild_train = len(dataset_index['B_class']['train']['wild'])
    b_wild_val = len(dataset_index['B_class']['val']['wild'])
    b_farmed_train = len(dataset_index['B_class']['train']['farmed'])
    b_farmed_val = len(dataset_index['B_class']['val']['farmed'])
    print(f"  Wild - Train: {b_wild_train}, Val: {b_wild_val}, Total: {b_wild_train + b_wild_val}")
    print(f"  Farmed - Train: {b_farmed_train}, Val: {b_farmed_val}, Total: {b_farmed_train + b_farmed_val}")
    print(f"  B-class Total - Train: {b_wild_train + b_farmed_train}, Val: {b_wild_val + b_farmed_val}")
    
    total_train = a_wild_train + a_farmed_train + b_wild_train + b_farmed_train
    total_val = a_wild_val + a_farmed_val + b_wild_val + b_farmed_val
    print(f"\nTotal - Train: {total_train}, Val: {total_val}, Total: {total_train + total_val}")

def main():
    print("=" * 60)
    print("Prepare Dataset")
    print("=" * 60)
    
    DATASET_DIR.mkdir(parents=True, exist_ok=True)
    
    all_samples = []
    
    print("\n[1/2] Prepare A-class data (structured light data)")
    a_samples = prepare_a_class_data(ORGANIZED_DIR, DATASET_DIR)
    all_samples.extend(a_samples)
    
    print("\n[2/2] Prepare B-class data (action camera data)")
    b_samples = prepare_b_class_data(DATASET_DIR)
    all_samples.extend(b_samples)
    
    print("\nCreate dataset index (8:2 train/val split)...")
    create_dataset_index(all_samples, DATASET_DIR, train_ratio=0.8)
    
    print("\n" + "=" * 60)
    print("Dataset preparation completed!")
    print("=" * 60)

if __name__ == "__main__":
    main()

